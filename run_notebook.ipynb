{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys, os, argparse, gc, time, numpy as np, tensorflow as tf, pandas as pd\n",
    "from leo_segmentation.utils import load_config, check_experiment, get_named_dict, \\\n",
    "                        log_data, load_yaml, train_logger, val_logger, print_to_string_io, \\\n",
    "                        save_pickled_data, model_dir, list_to_tensor, numpy_to_tensor, prepare_inputs,\\\n",
    "                        tensor_to_numpy\n",
    "from leo_segmentation.data import PascalDatagenerator, GeneralDatagenerator\n",
    "from run import train_model, evaluate_model\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir(\"./leo_segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "dataset = \"pascal_5i\"\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#leo = train_model(config, dataset, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in mode meta_val: ['bicycle', 'boat', 'aeroplane', 'bird', 'bottle']\n"
     ]
    }
   ],
   "source": [
    "mode = \"meta_val\"\n",
    "dataloader = PascalDatagenerator(dataset, data_type=mode)\n",
    "img_transformer = dataloader.transform_image\n",
    "mask_transformer = dataloader.transform_mask\n",
    "transformers = (img_transformer, mask_transformer)\n",
    "val_meta_data = dataloader.get_batch_data()\n",
    "class_in_metadata = val_meta_data[4]\n",
    "print(f\"classes in mode {mode}: {class_in_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class boat is selected\n"
     ]
    }
   ],
   "source": [
    "select_class = 1\n",
    "print(\"class\", class_in_metadata[select_class], \"is selected\")\n",
    "batch_data = get_named_dict(val_meta_data, select_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tr_imgs', 'tr_masks', 'val_imgs', 'val_masks'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected episode not availabe for loading\n",
      "Select one of the following: [100, 105, 110, 130, 135, 140, 145, 150, 155, 175, 180, 185, 190, 255, 260, 295, 355, 360, 400, 455, 60, 65, 70, 715, 725, 730, 740, 75, 765, 90, 95]740\n",
      "Episode 740 selected for Evaluation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (53).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f0308c1df589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\Study\\UHildesheim\\Project\\Scripts\\leo-srp-project\\gopika_leo_maml\\leo_segmentation\\run.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(dataset, support_imgs_or_paths, query_imgs_or_paths, support_masks_or_paths, dataloader)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mleo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_and_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"meta_val\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mseg_weight_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                 \u001b[0mleo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleo_inner_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             leo.finetuning_inner_loop(data_dict, features, seg_weight_grad,\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Study\\UHildesheim\\Project\\Scripts\\leo-srp-project\\gopika_leo_maml\\leo_segmentation\\leo_segmentation\\model.py\u001b[0m in \u001b[0;36mleo_inner_loop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0minner_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner_loop_lr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mlatents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_adaptation_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mlatents_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlatents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 948\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\temil\\onedrive\\documents\\codes_and_scripts\\python_envs\\pytorch-tf2-env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 2216\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (53)."
     ]
    }
   ],
   "source": [
    "prediction, leo, transformers = evaluate_model(dataset, batch_data.tr_imgs, batch_data.tr_masks, batch_data.val_imgs, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform forward_propagate and perform innerloop updates to generate weights\n",
    "seg_weight_grad, features = leo.leo_inner_loop(batch_data.tr_imgs, batch_data.tr_masks)\n",
    "#fine tune the weights\n",
    "tr_val_loss, seg_weight_grad, decoder_grads, mean_iou, weight = \\\n",
    "            leo.finetuning_inner_loop(batch_data, features, seg_weight_grad, transformers, mode)\n",
    "\n",
    "#prepare validation set\n",
    "if type(pred_data_index) == list:\n",
    "    input_embedding = np.vstack([list_to_tensor(batch_data.val_imgs[i], img_transformer) \\\n",
    "                                 for i in pred_data_index])\n",
    "    input_mask = np.vstack([list_to_tensor(batch_data.val_masks[i], mask_transformer) \\\n",
    "                            for i in pred_data_index])\n",
    "else:\n",
    "    input_embedding = list_to_tensor(batch_data.val_imgs[pred_data_index], img_transformer)\n",
    "    input_mask = list_to_tensor(batch_data.val_masks[pred_data_index], mask_transformer)\n",
    "\n",
    "input_embedding_for_plot = input_embedding.copy()\n",
    "input_mask_for_plot = input_mask.copy()\n",
    "\n",
    "input_embedding = prepare_inputs(numpy_to_tensor(input_embedding))\n",
    "input_mask = numpy_to_tensor(input_mask)\n",
    "#predict on training data\n",
    "skip_features = leo.forward_encoder(input_embedding)\n",
    "features = leo.forward_decoder(skip_features)\n",
    "val_prediction = leo.forward_segnetwork(features, input_embedding, weight)\n",
    "val_prediction = np.argmax(tensor_to_numpy(val_prediction), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_mask(embeds, preds, masks):\n",
    "    fig = plt.figure(figsize=(20,16))\n",
    "    for i in range(0, len(preds)):\n",
    "        embed = np.squeeze(embeds) if len(embeds) == 1 else embeds[i]\n",
    "        pred = np.squeeze(preds) if len(preds) == 1 else preds[i]\n",
    "        mask = np.squeeze(masks) if len(masks) == 1 else masks[i]\n",
    "        \n",
    "        fig.add_subplot(len(preds), 3, i*3+1)\n",
    "        plt.imshow((embed+1)/2)\n",
    "        plt.title(\"raw images\")\n",
    "        \n",
    "        fig.add_subplot(len(preds), 3, i*3+2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(\"ground_truth\")\n",
    "        \n",
    "        fig.add_subplot(len(preds), 3, i*3+3)\n",
    "        plt.imshow(pred, cmap=\"gray\")\n",
    "        plt.title(\"prediction\")\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_batch_mask(input_embedding_for_plot, val_prediction, input_mask_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = config.experiment.number\n",
    "experiment_description = config.experiment.description\n",
    "experiment_root_path = os.path.join(\"leo_segmentation\", \"data\", \"models\", f\"experiment_{experiment_number}\")\n",
    "cached_files = os.listdir(experiment_root_path)\n",
    "cached_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = cached_files[4]\n",
    "val_ious = pd.read_pickle(os.path.join(experiment_root_path, filename)).T\n",
    "print(f\"filename: {filename}\")\n",
    "filtered_scores = []\n",
    "for _class, _df in val_ious.iterrows():\n",
    "    if _class == \"episode\":\n",
    "        continue\n",
    "    temp = [i for i in _df if not np.isnan(i)]\n",
    "    class_sampling_frequency = len(temp)\n",
    "    mean_of_all_ious_per_class = np.mean(temp)\n",
    "    filtered_scores.append((_class, mean_of_all_ious_per_class, class_sampling_frequency))\n",
    "filtered_val_ious = pd.DataFrame(filtered_scores, columns=[\"classes\", \"mean_val_ious\", \"class_sampling_frequency\"])\n",
    "filtered_val_ious = filtered_val_ious.sort_values(by=\"mean_val_ious\", ascending=False)\n",
    "filtered_val_ious[filtered_val_ious.mean_val_ious > 0.7].count()\n",
    "filtered_val_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"leo_segmentation/data/fss1000/images\")[600:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_val_ious.class_sampling_frequency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ious = pd.read_pickle(os.path.join(experiment_root_path, cached_files[2])).T\n",
    "filtered_scores = []\n",
    "for _class, _df in train_ious.iterrows():\n",
    "    if _class == \"episode\":\n",
    "        continue\n",
    "    temp = [i for i in _df if not np.isnan(i)]\n",
    "    class_sampling_frequency = len(temp)\n",
    "    mean_of_all_ious_per_class = np.mean(temp)\n",
    "    filtered_scores.append((_class, mean_of_all_ious_per_class, class_sampling_frequency))\n",
    "filtered_train_ious = pd.DataFrame(filtered_scores, columns=[\"classes\", \"mean_train_ious\", \"class_sampling_frequency\"])\n",
    "filtered_train_ious = filtered_train_ious.sort_values(by=\"mean_train_ious\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes in meta-train and meta-val are unique if their intersection is zero\n",
    "set(filtered_train_ious.classes.unique()).intersection(set(filtered_val_ious.classes.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_val_loss = pd.read_pickle(os.path.join(experiment_root_path, cached_files[5]))\n",
    "meta_train_loss = pd.read_pickle(os.path.join(experiment_root_path, cached_files[3]))\n",
    "num_data_points = meta_val_loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"meta_losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(num_data_points), meta_val_loss.total_val_loss, label=\"meta_val_loss\")\n",
    "plt.plot(np.arange(num_data_points), meta_train_loss.total_val_loss, label=\"meta_train_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"leo_segmentation/data/models/experiment_15\"\n",
    "cached_files = os.listdir(results_path)\n",
    "cached_files = {i:k for i,k in enumerate(cached_files)}\n",
    "cached_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_pickle(os.path.join(results_path, cached_files[4]))\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "for i in range(5):\n",
    "    plt.plot(result_df.iloc[:,i], label=result_df.columns[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df = pd.read_pickle(os.path.join(results_path, cached_files[2]))\n",
    "train_results_df.fillna(0.0).max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_col={}\n",
    "for i in range(len(train_results_df.columns)):\n",
    "    if train_results_df.columns[i] == \"episode\":\n",
    "        continue\n",
    "    temp = train_results_df.iloc[:,i]\n",
    "    temp = temp.dropna()\n",
    "    counts_per_col[train_results_df.columns[i]] = len(temp)\n",
    "pd.Series(counts_per_col).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(pd.read_pickle(os.path.join(results_path,\n",
    "            cached_files[3]))[\"total_val_loss\"], label=\"train\")\n",
    "plt.plot(pd.read_pickle(os.path.join(results_path,\n",
    "            cached_files[5]))[\"total_val_loss\"], label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
