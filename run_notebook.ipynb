{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import sys, os, argparse, gc, time, numpy as np, tensorflow as tf, pandas as pd, seaborn as sns\n",
    "from leo_segmentation.utils import load_config, check_experiment, get_named_dict, \\\n",
    "                        log_data, load_yaml, train_logger, val_logger, print_to_string_io, \\\n",
    "                        save_pickled_data, model_dir, list_to_tensor, numpy_to_tensor, prepare_inputs,\\\n",
    "                        tensor_to_numpy\n",
    "from leo_segmentation.data import PascalDatagenerator, GeneralDatagenerator\n",
    "from run import train_model, evaluate_model\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir(\"./leo_segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "dataset = \"pascal_5i\"\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#leo = train_model(config, dataset, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"meta_val\"\n",
    "dataloader = PascalDatagenerator(dataset, data_type=mode)\n",
    "img_transformer = dataloader.transform_image\n",
    "mask_transformer = dataloader.transform_mask\n",
    "transformers = (img_transformer, mask_transformer)\n",
    "val_meta_data = dataloader.get_batch_data()\n",
    "class_in_metadata = val_meta_data[4]\n",
    "print(f\"classes in mode {mode}: {class_in_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_img(img):\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_class = 1\n",
    "print(\"class\", class_in_metadata[select_class], \"is selected\")\n",
    "batch_data = get_named_dict(val_meta_data, select_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_imgs = 5\n",
    "imgs = list_to_tensor(batch_data.val_imgs[:num_of_imgs], img_transformer)\n",
    "imgs = unnormalize_img(imgs)\n",
    "imgs = np.transpose(imgs, (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = list_to_tensor(batch_data.val_masks[:num_of_imgs], mask_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, leo, transformers = \\\n",
    "        evaluate_model(dataset, batch_data.tr_imgs, batch_data.tr_masks, batch_data.val_imgs[:num_of_imgs], dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_mask(imgs, groundtruth, prediction):\n",
    "    fig = plt.figure(figsize=(20,16))\n",
    "    imgs = np.expand_dims(imgs, 0) if imgs.ndim == 3 else imgs\n",
    "    groundtruth = np.expand_dims(groundtruth, 0) if groundtruth.ndim == 2 else groundtruth\n",
    "    prediction = np.expand_dims(prediction, 0) if prediction.ndim == 2 else prediction\n",
    "    \n",
    "    m = len(imgs)\n",
    "    for i in range(0, m):\n",
    "        fig.add_subplot(m, 3, i*3+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.title(\"raw images\")\n",
    "        \n",
    "        fig.add_subplot(m, 3, i*3+2)\n",
    "        plt.imshow(groundtruth[i], cmap=\"gray\")\n",
    "        plt.title(\"groundtruth\")\n",
    "        \n",
    "        fig.add_subplot(m, 3, i*3+3)\n",
    "        plt.imshow(prediction[i], cmap=\"gray\")\n",
    "        plt.title(\"prediction\")\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_batch_mask(imgs, groundtruth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./leo_segmentation/data/models/metatrain_results.csv\"\n",
    "df = pd.read_csv(data_path, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"Fold 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :-2]\n",
    "#df = pd.concat([df, ])\n",
    "#df = df.reset_index().rename(columns={'index':'epochs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = pd.concat([df.mean(axis=1), df.std(axis=1)], axis=1)\n",
    "stats = stats.reset_index().rename(columns={'index':'epochs', 0:'Loss', 1:'std_deviation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 50}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "means = stats.Loss.values\n",
    "std = stats.std_deviation.values\n",
    "lower = means - std\n",
    "upper = means + std\n",
    "fig = plt.figure(figsize=(30,12))\n",
    "plt.fill_between(np.arange(len(means)), lower, upper, color=\"#0090c1\", alpha=0.75)\n",
    "sns.lineplot(data=stats, x=\"epochs\", y=\"Loss\", color=\"#051014\", label=\"meta-train loss\")\n",
    "plt.title(\"Meta-train Loss\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = config.experiment.number\n",
    "experiment_description = config.experiment.description\n",
    "experiment_root_path = os.path.join(\"leo_segmentation\", \"data\", \"models\", f\"experiment_{experiment_number}\")\n",
    "cached_files = os.listdir(experiment_root_path)\n",
    "cached_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = cached_files[4]\n",
    "val_ious = pd.read_pickle(os.path.join(experiment_root_path, filename)).T\n",
    "print(f\"filename: {filename}\")\n",
    "filtered_scores = []\n",
    "for _class, _df in val_ious.iterrows():\n",
    "    if _class == \"episode\":\n",
    "        continue\n",
    "    temp = [i for i in _df if not np.isnan(i)]\n",
    "    class_sampling_frequency = len(temp)\n",
    "    mean_of_all_ious_per_class = np.mean(temp)\n",
    "    filtered_scores.append((_class, mean_of_all_ious_per_class, class_sampling_frequency))\n",
    "filtered_val_ious = pd.DataFrame(filtered_scores, columns=[\"classes\", \"mean_val_ious\", \"class_sampling_frequency\"])\n",
    "filtered_val_ious = filtered_val_ious.sort_values(by=\"mean_val_ious\", ascending=False)\n",
    "filtered_val_ious[filtered_val_ious.mean_val_ious > 0.7].count()\n",
    "filtered_val_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"leo_segmentation/data/fss1000/images\")[600:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_val_ious.class_sampling_frequency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ious = pd.read_pickle(os.path.join(experiment_root_path, cached_files[2])).T\n",
    "filtered_scores = []\n",
    "for _class, _df in train_ious.iterrows():\n",
    "    if _class == \"episode\":\n",
    "        continue\n",
    "    temp = [i for i in _df if not np.isnan(i)]\n",
    "    class_sampling_frequency = len(temp)\n",
    "    mean_of_all_ious_per_class = np.mean(temp)\n",
    "    filtered_scores.append((_class, mean_of_all_ious_per_class, class_sampling_frequency))\n",
    "filtered_train_ious = pd.DataFrame(filtered_scores, columns=[\"classes\", \"mean_train_ious\", \"class_sampling_frequency\"])\n",
    "filtered_train_ious = filtered_train_ious.sort_values(by=\"mean_train_ious\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes in meta-train and meta-val are unique if their intersection is zero\n",
    "set(filtered_train_ious.classes.unique()).intersection(set(filtered_val_ious.classes.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_val_loss = pd.read_pickle(os.path.join(experiment_root_path, cached_files[5]))\n",
    "meta_train_loss = pd.read_pickle(os.path.join(experiment_root_path, cached_files[3]))\n",
    "num_data_points = meta_val_loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.title(\"meta_losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(num_data_points), meta_val_loss.total_val_loss, label=\"meta_val_loss\")\n",
    "plt.plot(np.arange(num_data_points), meta_train_loss.total_val_loss, label=\"meta_train_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"leo_segmentation/data/models/experiment_15\"\n",
    "cached_files = os.listdir(results_path)\n",
    "cached_files = {i:k for i,k in enumerate(cached_files)}\n",
    "cached_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_pickle(os.path.join(results_path, cached_files[4]))\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "for i in range(5):\n",
    "    plt.plot(result_df.iloc[:,i], label=result_df.columns[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df = pd.read_pickle(os.path.join(results_path, cached_files[2]))\n",
    "train_results_df.fillna(0.0).max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_col={}\n",
    "for i in range(len(train_results_df.columns)):\n",
    "    if train_results_df.columns[i] == \"episode\":\n",
    "        continue\n",
    "    temp = train_results_df.iloc[:,i]\n",
    "    temp = temp.dropna()\n",
    "    counts_per_col[train_results_df.columns[i]] = len(temp)\n",
    "pd.Series(counts_per_col).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(pd.read_pickle(os.path.join(results_path,\n",
    "            cached_files[3]))[\"total_val_loss\"], label=\"train\")\n",
    "plt.plot(pd.read_pickle(os.path.join(results_path,\n",
    "            cached_files[5]))[\"total_val_loss\"], label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
